# Transform Architecture Discussion

> **Advanced Engineering Approaches for Data Transformations**

---

## ü§î Current Implementation Critique

### **Limitations of Current Approach**

The current implementation, while functional, has several engineering shortcomings:

#### 1. **String-Based Expression Parsing** (FilterTransform)
```scala
// Current: Naive string splitting
"age > 18".split("\\s+")  // Error-prone, no compile-time validation
```

**Problems**:
- ‚ùå No compile-time type safety
- ‚ùå Runtime parsing errors
- ‚ùå Limited expression complexity
- ‚ùå Hard to extend with new operators
- ‚ùå No expression optimization
- ‚ùå Poor error messages

#### 2. **Manual Field Operations** (MapTransform)
```scala
// Current: Imperative field manipulation
var transformedData = record.data
config.mappings.foreach { case (sourceField, targetField) => ... }
```

**Problems**:
- ‚ùå Not composable
- ‚ùå Difficult to reason about
- ‚ùå No type-level guarantees
- ‚ùå Can't leverage optics/lenses
- ‚ùå Mutation-based (var)

#### 3. **Basic String Manipulation** (FlatMapTransform)
```scala
// Current: String splitting with manual ID generation
value.split(",").map(_.trim)
```

**Problems**:
- ‚ùå Hardcoded delimiter
- ‚ùå No support for complex nested structures
- ‚ùå Manual metadata management

#### 4. **No Operator Overloading**
```scala
// Can't do: record.field("age") > 18
// Can't do: record.field("name").map(_.toUpperCase)
```

#### 5. **Limited Composability**
```scala
// Current: Sequential application
transforms.foldLeft(flow) { (f, t) => f.via(t.flow) }

// Can't do: filter.andThen(map).or(alternative)
```

---

## üéØ Proposed Advanced Architectures

### **Approach 1: Type-Safe Expression DSL with Implicit Conversions**

#### **Concept**
Build a compile-time type-safe DSL for expressions using Scala's implicit conversions and operator overloading.

#### **Example API**
```scala
import com.dataflow.transforms.dsl._

// Type-safe field references
val age = Field[Int]("age")
val status = Field[String]("status")
val premium = Field[Boolean]("premium")

// Compile-time type-checked expressions
val filter = Transform.filter(
  (age > 18) && (status === "active") || premium
)

// Type-safe field transformations
val map = Transform.map(
  Field("firstName").as("first_name"),
  Field("lastName").as("last_name"),
  Field("email").lowercase,
  Field("password").delete
)

// Nested field access with optics
val extract = Transform.map(
  Field("user.profile.email").as("email"),
  Field("user.permissions").as("permissions")
)

// Fluent composition
val pipeline = filter
  .andThen(map)
  .andThen(Transform.flatMap("items"))
  .withErrorHandling(Strategy.Skip)
```

#### **Implementation Strategy**

**1. Field Type Class**
```scala
sealed trait Field[A] {
  def name: String

  // Comparison operators
  def ===(value: A): Predicate
  def !==(value: A): Predicate
  def >(value: A)(implicit ord: Ordering[A]): Predicate
  def <(value: A)(implicit ord: Ordering[A]): Predicate

  // Transformations
  def as(newName: String): FieldMapping
  def map[B](f: A => B): FieldMapping
  def delete: FieldMapping

  // Optics for nested access
  def lens: Lens[DataRecord, Option[A]]
}

object Field {
  def apply[A](name: String): Field[A] = new FieldImpl[A](name)

  // Implicit conversions for literals
  implicit def stringToField(name: String): Field[String] = Field[String](name)
}
```

**2. Predicate Algebra**
```scala
sealed trait Predicate {
  def &&(other: Predicate): Predicate = And(this, other)
  def ||(other: Predicate): Predicate = Or(this, other)
  def unary_! : Predicate = Not(this)
}

case class Equals[A](field: Field[A], value: A) extends Predicate
case class GreaterThan[A](field: Field[A], value: A)(implicit ord: Ordering[A]) extends Predicate
case class And(left: Predicate, right: Predicate) extends Predicate
case class Or(left: Predicate, right: Predicate) extends Predicate
case class Not(pred: Predicate) extends Predicate
```

**3. Transform DSL**
```scala
object Transform {
  def filter(predicate: Predicate): FilterTransform =
    new FilterTransform(predicate.compile())

  def map(mappings: FieldMapping*): MapTransform =
    new MapTransform(mappings.toSeq)

  def flatMap[A](field: Field[Seq[A]]): FlatMapTransform =
    new FlatMapTransform(field)
}

// Compile predicate to efficient evaluation
trait Predicate {
  def compile(): DataRecord => Boolean = {
    // Generate optimized evaluation function
    // Could use macros for compile-time optimization
  }
}
```

**Pros**:
- ‚úÖ Compile-time type safety
- ‚úÖ Natural, readable syntax
- ‚úÖ IDE autocomplete support
- ‚úÖ Composable expressions
- ‚úÖ Can optimize at compile time with macros

**Cons**:
- ‚ö†Ô∏è More complex implementation
- ‚ö†Ô∏è Learning curve for DSL
- ‚ö†Ô∏è Requires type evidence for all fields

---

### **Approach 2: Lens-Based Transformations (Monocle/Quicklens)**

#### **Concept**
Use optics (lenses, prisms, traversals) for type-safe, composable field access and transformation.

#### **Example API**
```scala
import monocle.macros.GenLens
import com.dataflow.transforms.optics._

// Define record schema with lenses
case class UserRecord(
  firstName: String,
  lastName: String,
  age: Int,
  email: String
)

// Automatic lens generation
val firstName = GenLens[UserRecord](_.firstName)
val age = GenLens[UserRecord](_.age)

// Lens-based transformations
val transform = Transform.lensMap[UserRecord]
  .modify(firstName)(_.capitalize)
  .modify(age)(_ + 1)
  .rename(firstName, "first_name")
  .delete(_.password)

// Traversals for nested structures
val emailLens = GenLens[UserRecord](_.email)
val transform = Transform.traversal(emailLens).lowercase

// Prisms for optional fields
val premiumPrism = Prism[User, PremiumUser]
val transform = Transform.prism(premiumPrism)
  .map(_.upgradeTier)
```

**Pros**:
- ‚úÖ Type-safe at compile time
- ‚úÖ Composable transformations
- ‚úÖ Mature library (Monocle)
- ‚úÖ Powerful for nested structures
- ‚úÖ Lawful (category theory)

**Cons**:
- ‚ö†Ô∏è Requires case classes (not Map[String, String])
- ‚ö†Ô∏è Steep learning curve
- ‚ö†Ô∏è Runtime overhead of optics
- ‚ö†Ô∏è Doesn't work well with dynamic schemas

---

### **Approach 3: Type Class Based Extensible Transforms**

#### **Concept**
Define transformations as type classes, making the system open for extension.

#### **Example API**
```scala
// Type class for transformable types
trait Transformable[A] {
  def filter(a: A, predicate: A => Boolean): Option[A]
  def map[B](a: A)(f: A => B)(implicit tb: Transformable[B]): B
  def flatMap[B](a: A)(f: A => Seq[B])(implicit tb: Transformable[B]): Seq[B]
}

// Instances for DataRecord
implicit val dataRecordTransformable: Transformable[DataRecord] =
  new Transformable[DataRecord] {
    def filter(rec: DataRecord, pred: DataRecord => Boolean): Option[DataRecord] =
      if (pred(rec)) Some(rec) else None

    def map[B](rec: DataRecord)(f: DataRecord => B)(implicit tb: Transformable[B]): B =
      f(rec)

    def flatMap[B](rec: DataRecord)(f: DataRecord => Seq[B])(implicit tb: Transformable[B]): Seq[B] =
      f(rec)
  }

// Usage with context bounds
def pipeline[A: Transformable](data: A): A = {
  val T = implicitly[Transformable[A]]
  T.filter(data, _.field("age").toInt > 18)
    .map(T.map(_)(_.withField("status", "active")))
    .getOrElse(data)
}

// Extensible - users can add their own instances
implicit val customTransformable: Transformable[CustomType] = ...
```

**Pros**:
- ‚úÖ Open for extension
- ‚úÖ Type-safe polymorphism
- ‚úÖ Testable (mock instances)
- ‚úÖ Follows functional patterns

**Cons**:
- ‚ö†Ô∏è Complex for beginners
- ‚ö†Ô∏è Implicit resolution can be tricky
- ‚ö†Ô∏è More boilerplate

---

### **Approach 4: Free Monad / Tagless Final**

#### **Concept**
Represent transformations as an algebra (AST), separate from interpretation.

#### **Example API**
```scala
// Transformation algebra
sealed trait TransformOp[A]
case class Filter(predicate: Predicate) extends TransformOp[DataRecord]
case class Map(mappings: Seq[FieldMapping]) extends TransformOp[DataRecord]
case class FlatMap(field: String) extends TransformOp[Seq[DataRecord]]

// Free monad over the algebra
type TransformF[A] = Free[TransformOp, A]

// DSL constructors
def filter(pred: Predicate): TransformF[DataRecord] =
  Free.liftF(Filter(pred))

def map(mappings: FieldMapping*): TransformF[DataRecord] =
  Free.liftF(Map(mappings))

// Program as value
val program: TransformF[DataRecord] = for {
  filtered <- filter(age > 18)
  mapped <- map(Field("name").as("full_name"))
  split <- flatMap("items")
} yield split

// Multiple interpreters
val streamInterpreter: TransformOp ~> Flow[DataRecord, ?, NotUsed]
val testInterpreter: TransformOp ~> Id
val optimizingInterpreter: TransformOp ~> TransformOp // Rewrite rules

// Run with chosen interpreter
val flow = program.foldMap(streamInterpreter)
```

**Pros**:
- ‚úÖ Separate description from execution
- ‚úÖ Multiple interpreters (testing, optimization, etc.)
- ‚úÖ Composable programs
- ‚úÖ Can optimize/rewrite before execution

**Cons**:
- ‚ö†Ô∏è Very advanced concept
- ‚ö†Ô∏è Steep learning curve
- ‚ö†Ô∏è May be overkill for this use case
- ‚ö†Ô∏è Runtime overhead of Free

---

### **Approach 5: Macro-Based Compile-Time Code Generation**

#### **Concept**
Use Scala macros to generate optimized transformation code at compile time.

#### **Example API**
```scala
import scala.language.experimental.macros
import com.dataflow.transforms.macros._

// Macro generates optimized code at compile time
val transform = Transform.compile {
  filter(r => r.field[Int]("age") > 18 && r.field[String]("status") == "active")
    .map(r => r.rename("firstName" -> "first_name"))
    .flatMap("items")
}

// Macro expands to:
// Flow[DataRecord].mapConcat { record =>
//   val age = record.data.get("age").map(_.toInt).getOrElse(0)
//   val status = record.data.get("status").getOrElse("")
//   if (age > 18 && status == "active") {
//     val renamed = record.data - "firstName" + ("first_name" -> record.data("firstName"))
//     val items = renamed.get("items").getOrElse("").split(",")
//     items.map(item => record.copy(data = renamed + ("item" -> item)))
//   } else Nil
// }

// Type-checked at compile time, zero runtime overhead
```

**Pros**:
- ‚úÖ Zero runtime overhead
- ‚úÖ Type-checked at compile time
- ‚úÖ Generated code is optimized
- ‚úÖ Can validate expressions early

**Cons**:
- ‚ö†Ô∏è Very complex to implement
- ‚ö†Ô∏è Macros are being replaced in Scala 3
- ‚ö†Ô∏è Debugging macro errors is hard
- ‚ö†Ô∏è IDE support may be limited

---

### **Approach 6: Shapeless Generic Programming**

#### **Concept**
Use Shapeless for generic transformations that work with any case class structure.

#### **Example API**
```scala
import shapeless._
import com.dataflow.transforms.generic._

// Works with any case class
case class User(firstName: String, lastName: String, age: Int)

// Generic transformations
val transform = Transform.generic[User]
  .rename('firstName, 'first_name)  // Compile-time field checking
  .delete('password)                // Compile error if field doesn't exist
  .map('age)(_ + 1)

// HList-based transformations
val user = User("John", "Doe", 25)
val hlist = Generic[User].to(user)  // String :: String :: Int :: HNil
val transformed = hlist.map(polymorphicFunction)
val back = Generic[User].from(transformed)

// Generic field access
def getField[A, K <: Symbol, V](a: A, field: Witness.Aux[K])
  (implicit selector: Selector[A, K]): selector.Out =
  selector(a)
```

**Pros**:
- ‚úÖ Works with any case class
- ‚úÖ Type-safe at compile time
- ‚úÖ Powerful for generic operations
- ‚úÖ No runtime reflection

**Cons**:
- ‚ö†Ô∏è Very steep learning curve
- ‚ö†Ô∏è Complex error messages
- ‚ö†Ô∏è Compilation time impact
- ‚ö†Ô∏è Shapeless 2 vs 3 migration issues

---

## üîÑ Hybrid Approach (Recommended)

### **Combine Best of Multiple Approaches**

```scala
// Layer 1: Type-safe DSL for common cases
val simpleFilter = Transform.filter(Field[Int]("age") > 18)

// Layer 2: Advanced users can use optics
val lensTransform = Transform.lens(userLens.age).modify(_ + 1)

// Layer 3: Fallback to string-based for dynamic schemas
val dynamicFilter = Transform.filter("age > 18")  // Runtime parsing

// All layers compile to the same underlying Flow
trait Transform {
  def toFlow: Flow[DataRecord, DataRecord, NotUsed]
}
```

**Architecture**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     DSL Layer (Type-Safe API)        ‚îÇ
‚îÇ  Field[A], Predicate, Operators      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Algebra Layer (Expression AST)     ‚îÇ
‚îÇ  FilterExpr, MapExpr, FlatMapExpr    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Compilation Layer (Optimization)    ‚îÇ
‚îÇ  Constant folding, Dead code elim.   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Execution Layer (Pekko Streams)     ‚îÇ
‚îÇ  Flow[DataRecord, DataRecord, ...]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Comparison Matrix

| Approach | Type Safety | Readability | Extensibility | Complexity | Performance |
|----------|-------------|-------------|---------------|------------|-------------|
| **Current** | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **DSL + Implicits** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Lens-Based** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Type Classes** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Free Monad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Macros** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Shapeless** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Hybrid** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üéØ Recommendation

### **Phase 1: Type-Safe DSL (Approach 1)**

Start with a type-safe DSL using implicit conversions:

**Why**:
1. ‚úÖ Significant improvement over current approach
2. ‚úÖ Reasonable complexity
3. ‚úÖ Great developer experience
4. ‚úÖ Can be built incrementally

**Implementation Priority**:
```scala
// Week 1: Field DSL
Field[Int]("age") > 18
Field[String]("status") === "active"

// Week 2: Predicate Algebra
(age > 18) && (status === "active")

// Week 3: Transform DSL
Transform.filter(...).andThen(Transform.map(...))

// Week 4: Optimization
Compile predicates to efficient evaluation functions
```

### **Phase 2: Add Optics Support (Approach 2)**

For users with complex nested structures:

```scala
// Optional advanced API
import com.dataflow.transforms.optics._
val transform = Transform.lens(userLens.email).lowercase
```

### **Phase 3: Extensibility (Approach 3)**

Allow users to extend with custom transforms via type classes.

---

## üí° Migration Path

### **Keep Current API for Backwards Compatibility**

```scala
// Old API still works
new FilterTransform(FilterConfig("age > 18"))

// New DSL API
Transform.filter(Field[Int]("age") > 18)

// Both compile to same underlying Flow
```

### **Gradual Migration**

```scala
object Transform {
  // New DSL API
  def filter(predicate: Predicate): FilterTransform = ???

  // Backwards compatibility
  @deprecated("Use DSL: Transform.filter(Field(...) > 18)")
  def filter(expression: String): FilterTransform =
    FilterTransform(FilterConfig(expression))
}
```

---

## üìñ Example: Complete Pipeline with DSL

```scala
import com.dataflow.transforms.dsl._

// Define fields with types
val age = Field[Int]("age")
val status = Field[String]("status")
val firstName = Field[String]("firstName")
val email = Field[String]("email")
val items = Field[Seq[String]]("items")

// Build type-safe pipeline
val pipeline = Transform.pipeline()
  // Type-checked filter
  .filter((age >= 18) && (status === "active"))

  // Type-safe field mappings
  .map(
    firstName.as("first_name"),
    email.lowercase.as("email"),
    Field("password").delete
  )

  // Nested field access
  .extract(
    Field("user.profile.avatar").as("avatar_url")
  )

  // Array operations
  .flatMap(items, "item")

  // Error handling
  .withErrorHandling(Strategy.Skip)

  // Compile to Pekko Streams Flow
  .toFlow

// Usage
Source(records)
  .via(pipeline)
  .runWith(sink)
```

---

## ü§î Questions for Discussion

1. **Approach Preference**: Which approach resonates most with the team?
2. **Type Safety vs Simplicity**: How much complexity are we willing to accept?
3. **Migration Strategy**: Big bang rewrite or gradual migration?
4. **Dynamic Schemas**: Do we need to support Map[String, String] or can we move to case classes?
5. **Learning Curve**: What's acceptable for new team members?
6. **Performance**: How critical is zero-overhead abstraction?
7. **External Libraries**: Are we comfortable with Monocle, Shapeless, Cats?

---

## üìö References

- **Monocle**: https://www.optics.dev/Monocle/
- **Shapeless**: https://github.com/milessabin/shapeless
- **Free Monads**: "Free Monads in Cats" by Daniel Spiewak
- **Type Classes**: "Type Classes in Scala" by Daniel Westheide
- **DSL Design**: "Scala DSL Design" by Debasish Ghosh
- **Macros**: "Scala Macros" (Scala 2) / "Scala 3 Metaprogramming"

---

**Let's discuss which direction makes the most sense for your engineering goals!** üöÄ
