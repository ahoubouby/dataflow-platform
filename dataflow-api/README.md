# dataflow-api

> **Application Module**: Cluster runtime, Cassandra persistence, HTTP API, and pipeline execution orchestration

---

## üéØ **Purpose**

**dataflow-api** is the **application module** that brings everything together. It provides:

- ‚úÖ **Cluster runtime** (Pekko Cluster with sharding)
- ‚úÖ **Cassandra persistence** (event journal and snapshots)
- ‚úÖ **HTTP REST API** (pipeline management)
- ‚úÖ **Pipeline execution** (Source ‚Üí Transform ‚Üí Sink orchestration)
- ‚úÖ **Metrics & monitoring** (Kamon/Prometheus)
- ‚úÖ **WebSocket** (real-time updates)

**IMPORTANT**: This is an **application**, not a library!

---

## ‚ú® **What This Module Contains**

- ‚úÖ **Cluster dependencies** (`pekko-cluster-typed`, `pekko-cluster-sharding-typed`)
- ‚úÖ **Cassandra driver** (`pekko-persistence-cassandra`, `java-driver-core`)
- ‚úÖ **HTTP server** (`pekko-http`)
- ‚úÖ **Metrics collection** (`kamon-core`, `kamon-prometheus`)
- ‚úÖ **Execution orchestration** (`PipelineExecutor`, `ExecutionOrchestrator`)
- ‚úÖ **Main entry point** (`ApiMain.scala`)

**Why?** This is the **application layer** that runs the distributed system. It depends on dataflow-core (domain library) but adds all the infrastructure.

---

## üèóÔ∏è **Architecture**

### **Dependency Hierarchy**

```
dataflow-api (THIS MODULE - APPLICATION)
    ‚îú‚îÄ‚îÄ depends on ‚Üí dataflow-core (domain library)
    ‚îú‚îÄ‚îÄ depends on ‚Üí dataflow-sources (connectors)
    ‚îú‚îÄ‚îÄ depends on ‚Üí dataflow-transforms (processing)
    ‚îî‚îÄ‚îÄ depends on ‚Üí dataflow-sinks (output)

Adds:
    ‚îú‚îÄ‚îÄ Cluster runtime (Pekko Cluster)
    ‚îú‚îÄ‚îÄ Cassandra client (persistence)
    ‚îú‚îÄ‚îÄ HTTP API (management)
    ‚îú‚îÄ‚îÄ Execution orchestration
    ‚îî‚îÄ‚îÄ Metrics (Kamon)
```

### **Module Structure**

```
dataflow-api/
‚îú‚îÄ‚îÄ src/main/scala/com/dataflow/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ApiMain.scala                    # üöÄ Application entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/                          # API DTOs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CreatePipelineRequest.scala
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PipelineResponse.scala
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ErrorResponse.scala
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/                          # HTTP routes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PipelineRoutes.scala         # CRUD operations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HealthRoutes.scala           # Health checks
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MetricsRoutes.scala          # Metrics endpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ http/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ HttpServer.scala             # HTTP server setup
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ execution/                           # üÜï Pipeline execution
‚îÇ       ‚îú‚îÄ‚îÄ PipelineExecutor.scala           # Runs Source ‚Üí Transform ‚Üí Sink
‚îÇ       ‚îú‚îÄ‚îÄ ExecutionOrchestrator.scala      # Manages executor lifecycle
‚îÇ       ‚îú‚îÄ‚îÄ PipelineEventListener.scala      # Reads events from Cassandra
‚îÇ       ‚îú‚îÄ‚îÄ TransformConfigMapper.scala      # Maps configs to transforms
‚îÇ       ‚îî‚îÄ‚îÄ SinkFactory.scala                # Creates sink instances
‚îÇ
‚îî‚îÄ‚îÄ src/main/resources/
    ‚îú‚îÄ‚îÄ application.conf                     # ‚úÖ ALL cluster/Cassandra config
    ‚îú‚îÄ‚îÄ cluster.conf                         # Cluster settings
    ‚îú‚îÄ‚îÄ kamon-local.conf                     # Metrics configuration
    ‚îî‚îÄ‚îÄ logback.xml                          # Logging configuration
```

---

## üì¶ **Dependencies**

**build.sbt** (complete):

```scala
lazy val dataflowApi = (project in file("dataflow-api"))
  .dependsOn(
    dataflowCore % "compile->compile;test->test",
    dataflowSources % "compile->compile",
    dataflowTransforms % "compile->compile",
    dataflowSinks % "compile->compile"
  )
  .settings(
    libraryDependencies ++=
      commonDependencies ++
      testDependencies ++
      httpDependencies ++           // Pekko HTTP
      validationDependencies ++
      metricsDependencies ++        // ‚úÖ Kamon metrics
      clusterDependencies ++        // ‚úÖ Pekko Cluster
      persistenceDependencies       // ‚úÖ Cassandra driver
  )
```

**Key Dependencies:**
- ‚úÖ `clusterDependencies` - Cluster sharding, split-brain resolution
- ‚úÖ `persistenceDependencies` - Cassandra driver, persistence plugins
- ‚úÖ `metricsDependencies` - Kamon, Prometheus reporter
- ‚úÖ `httpDependencies` - Pekko HTTP, CORS support

---

## ‚öôÔ∏è **Configuration**

### **application.conf** (comprehensive)

**This module contains ALL configuration** for:

1. **Cluster Configuration**
```hocon
pekko {
  actor.provider = cluster  # ‚úÖ Runs as a cluster

  remote.artery {
    canonical {
      hostname = "127.0.0.1"
      port = 2551
    }
  }

  cluster {
    seed-nodes = ["pekko://DataFlowSystem@127.0.0.1:2551"]

    sharding {
      number-of-shards = 100
    }
  }
}
```

2. **Cassandra Persistence**
```hocon
pekko.persistence.cassandra {
  journal {
    keyspace = "dataflow_journal"
    table = "messages"
    keyspace-autocreate = true
    tables-autocreate = true
  }

  snapshot {
    keyspace = "dataflow_snapshot"
    table = "snapshots"
  }
}

datastax-java-driver {
  basic {
    contact-points = ["127.0.0.1:9042"]
    load-balancing-policy.local-datacenter = "datacenter1"
  }

  advanced {
    reconnection-policy {
      class = ExponentialReconnectionPolicy
      base-delay = 1 second
      max-delay = 60 seconds
    }
  }
}
```

3. **Kamon Metrics**
```hocon
include "kamon-local.conf"

kamon {
  prometheus {
    embedded-server {
      hostname = "0.0.0.0"
      port = 9095
    }
  }
}
```

4. **HTTP API**
```hocon
dataflow.api {
  host = "0.0.0.0"
  port = 8080
}

pekko.http {
  server {
    request-timeout = 30s
    idle-timeout = 60s
  }
}
```

---

## üöÄ **Running the Application**

### **Prerequisites**

1. **Start Infrastructure**:
```bash
cd docker
docker-compose up -d
```

This starts:
- Cassandra (port 9042)
- Kafka (port 9093)
- PostgreSQL (port 5432)
- Elasticsearch (port 9200)
- Grafana (port 3000)
- Prometheus (port 9090)

2. **Initialize Cassandra**:
```bash
cd docker/cassandra-init
./init-cassandra.sh
```

Or manually:
```bash
docker exec -i dataflow-cassandra cqlsh < docker/cassandra-init/01-init-keyspaces.cql
```

3. **Wait for Cassandra** (use the wait script):
```bash
./scripts/wait-for-cassandra.sh
```

### **Start the Application**

```bash
sbt "project dataflow-api" run
```

**Expected output:**
```
[INFO] Starting DataFlow Platform API...
[INFO] Cluster bootstrap starting
[INFO] Pekko Management started on http://127.0.0.1:8558
[INFO] Cassandra session initialized
[INFO] Cluster joined, member status: Up
[INFO] Initializing PipelineAggregate sharding
[INFO] Starting ExecutionOrchestrator
[INFO] PipelineEventListener started successfully
[INFO] Kamon metrics reporter started on http://0.0.0.0:9095/metrics
[INFO] HTTP server online at http://0.0.0.0:8080/
[INFO] DataFlow Platform API started successfully!
```

### **Verify Services**

```bash
# Health check
curl http://localhost:8080/health

# Metrics (Prometheus format)
curl http://localhost:9095/metrics

# List pipelines
curl http://localhost:8080/api/v1/pipelines
```

---

## üé≠ **Architecture Components**

### **1. ApiMain (Entry Point)**

The main application entry point:

```scala
object ApiMain extends App {
  // Initialize Kamon metrics
  Kamon.init()

  // Create actor system
  val system = ActorSystem[Nothing](Behaviors.setup[Nothing] { context =>
    // Initialize cluster sharding
    val sharding = ClusterSharding(system)
    sharding.init(Entity(PipelineAggregate.TypeKey) { entityContext =>
      PipelineAggregate(entityContext.entityId)
    })

    // Start execution orchestrator
    val orchestrator = context.spawn(ExecutionOrchestrator(), "execution-orchestrator")

    // Start event listener (reads from Cassandra)
    PipelineEventListener.start(orchestrator)

    // Start HTTP server
    val routes = new PipelineRoutes(sharding)
    HttpServer.start(routes.routes)

    Behaviors.empty
  }, "DataFlowSystem")

  // Graceful shutdown
  sys.addShutdownHook {
    Kamon.stop()
    system.terminate()
  }
}
```

### **2. Pipeline Execution Flow**

```
1. HTTP POST /api/v1/pipelines/{id}/start
   ‚Üì
2. PipelineRoutes ‚Üí PipelineAggregate (via cluster sharding)
   ‚Üì
3. PipelineAggregate emits PipelineStarted event ‚Üí Cassandra
   ‚Üì
4. PipelineEventListener reads event from Cassandra journal
   ‚Üì
5. ExecutionOrchestrator spawns PipelineExecutor
   ‚Üì
6. PipelineExecutor builds Pekko Streams graph:
   Source ‚Üí Transform(s) ‚Üí Sink
   ‚Üì
7. Data flows through pipeline
   ‚Üì
8. Metrics collected via Kamon
```

### **3. ExecutionOrchestrator**

Manages the lifecycle of all pipeline executors:

```scala
object ExecutionOrchestrator {
  sealed trait Command
  case class HandleEvent(event: Event, pipelineId: String) extends Command

  def apply(): Behavior[Command] = {
    Behaviors.setup { context =>
      Behaviors.receiveMessage {
        case HandleEvent(PipelineStarted(id, _), pipelineId) =>
          // Spawn PipelineExecutor
          val executor = context.spawn(PipelineExecutor(pipelineId), s"executor-$pipelineId")
          executor ! PipelineExecutor.Start
          Behaviors.same

        case HandleEvent(PipelineStopped(id, _, _, _), pipelineId) =>
          // Stop executor
          context.child(s"executor-$pipelineId").foreach(context.stop)
          Behaviors.same
      }
    }
  }
}
```

### **4. PipelineEventListener**

Reads events from Cassandra and forwards to orchestrator:

```scala
object PipelineEventListener {
  def start(orchestrator: ActorRef[ExecutionOrchestrator.Command])(implicit system: ActorSystem[_]): Future[Unit] = {
    // Create Cassandra read journal
    val readJournal = PersistenceQuery(system)
      .readJournalFor[CassandraReadJournal](CassandraReadJournal.Identifier)

    // Stream events with tag "pipeline"
    RestartSource.withBackoff(minBackoff = 3.seconds, maxBackoff = 30.seconds) { () =>
      readJournal.eventsByTag("pipeline", Offset.noOffset)
        .map { envelope =>
          val pipelineId = envelope.persistenceId.split("-", 2).lastOption.getOrElse("unknown")
          orchestrator ! ExecutionOrchestrator.HandleEvent(envelope.event, pipelineId)
        }
    }.runWith(Sink.ignore)
  }
}
```

### **5. PipelineExecutor**

Actually runs pipelines (Source ‚Üí Transform ‚Üí Sink):

```scala
class PipelineExecutor(pipelineId: String) {
  def buildGraph(config: PipelineConfig): RunnableGraph[KillSwitch] = {
    val source = SourceFactory.create(config.source)
    val transforms = config.transforms.map(TransformConfigMapper.toTransform)
    val sink = SinkFactory.create(config.sink)

    // Build stream: Source ‚Üí Transform(s) ‚Üí Sink
    val streamSource = source.stream()
    val transformedStream = transforms.foldLeft(streamSource) { (stream, transform) =>
      stream.via(transform.flow)
    }

    transformedStream
      .viaMat(KillSwitches.single)(Keep.right)
      .toMat(sink.sink)(Keep.left)
  }
}
```

---

## üì° **HTTP API**

### **Endpoints**

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/pipelines` | Create pipeline |
| GET | `/api/v1/pipelines` | List all pipelines |
| GET | `/api/v1/pipelines/{id}` | Get pipeline details |
| POST | `/api/v1/pipelines/{id}/start` | Start pipeline |
| POST | `/api/v1/pipelines/{id}/stop` | Stop pipeline |
| POST | `/api/v1/pipelines/{id}/pause` | Pause pipeline |
| POST | `/api/v1/pipelines/{id}/resume` | Resume pipeline |
| GET | `/api/v1/pipelines/{id}/metrics` | Get metrics |
| GET | `/health` | Health check |

### **Example Usage**

```bash
# Create a pipeline
curl -X POST http://localhost:8080/api/v1/pipelines \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test Pipeline",
    "description": "File to console pipeline",
    "source": {
      "sourceType": "file",
      "connectionString": "/data/input.csv",
      "batchSize": 100
    },
    "transforms": [
      {
        "transformType": "filter",
        "config": {"field": "status", "value": "active"}
      }
    ],
    "sink": {
      "sinkType": "console",
      "connectionString": "",
      "batchSize": 10
    }
  }'

# Start the pipeline
curl -X POST http://localhost:8080/api/v1/pipelines/{pipeline-id}/start

# Get metrics
curl http://localhost:8080/api/v1/pipelines/{pipeline-id}/metrics
```

---

## üìä **Metrics & Monitoring**

### **Kamon Metrics**

Metrics are exposed at `http://localhost:9095/metrics` in Prometheus format:

```
# HELP jvm_memory_used_bytes JVM memory used
# TYPE jvm_memory_used_bytes gauge
jvm_memory_used_bytes{area="heap"} 1.23456789e8

# HELP pekko_actor_mailbox_size Actor mailbox size
# TYPE pekko_actor_mailbox_size gauge
pekko_actor_mailbox_size{actor="pipeline-123"} 5

# HELP dataflow_pipeline_records_processed_total Total records processed
# TYPE dataflow_pipeline_records_processed_total counter
dataflow_pipeline_records_processed_total{pipeline="test-pipeline"} 10000
```

### **Grafana Dashboards**

Access Grafana at `http://localhost:3000` (admin/admin):

- JVM metrics (heap, threads, GC)
- Actor metrics (mailbox size, processing time)
- Pipeline metrics (throughput, latency)
- System metrics (CPU, memory)

---

## üß™ **Testing**

### **Run Tests**

```bash
sbt "project dataflow-api" test
```

### **Integration Tests**

```bash
sbt "project dataflow-api" it:test
```

### **End-to-End Test**

```bash
# Start infrastructure
docker-compose up -d

# Wait for Cassandra
./scripts/wait-for-cassandra.sh

# Run application
sbt "project dataflow-api" run

# In another terminal, run tests
./scripts/api-usage-examples.sh
```

---

## üîß **Troubleshooting**

### **Cassandra Connection Issues**

**Error**: `Could not reach any contact point /127.0.0.1:9042`

**Solution**:
```bash
# 1. Check Cassandra is running
docker ps | grep cassandra

# 2. Wait for Cassandra to be ready (takes 60s)
./scripts/wait-for-cassandra.sh

# 3. Verify connectivity
docker exec dataflow-cassandra cqlsh -e "describe keyspaces"
```

### **Cluster Not Forming**

**Error**: `Cluster node not joining`

**Solution**:
- Check seed nodes in `application.conf`
- Ensure port 2551 is available
- Check firewall settings

### **HTTP Server Won't Start**

**Error**: `Address already in use: 8080`

**Solution**:
```bash
# Change port via environment variable
API_PORT=8081 sbt "project dataflow-api" run
```

---

## ü§î **Why This Architecture?**

### **Separation of Concerns**

- **dataflow-core**: Pure domain logic (library)
- **dataflow-api**: Infrastructure + runtime (application)

This allows:
- ‚úÖ Testing domain logic without cluster
- ‚úÖ Reusing domain in different contexts
- ‚úÖ Clear dependency boundaries
- ‚úÖ No circular dependencies

### **Event-Driven Orchestration**

Instead of spawning executors directly from aggregates:

```scala
// ‚ùå BAD: Aggregate spawns executors (wrong layer)
case PipelineStarted(...) =>
  val executor = context.spawn(PipelineExecutor(...))  // Breaks separation!

// ‚úÖ GOOD: Event-driven orchestration
case PipelineStarted(...) =>
  persist(event)  // Pure event sourcing in aggregate

// Separately:
PipelineEventListener reads event ‚Üí ExecutionOrchestrator spawns executor
```

---

## üîó **Related Modules**

| Module | Relationship |
|--------|-------------|
| **dataflow-core** | **Domain library** - This module depends on it |
| **dataflow-sources** | **Connectors** - This module depends on it |
| **dataflow-transforms** | **Processing** - This module depends on it |
| **dataflow-sinks** | **Output** - This module depends on it |

---

## üöÄ **Next Steps**

- See [Main README.md](../README.md) for quick start
- See [API_DOCUMENTATION.md](../docs/API_DOCUMENTATION.md) for complete API reference
- See [ARCHITECTURE_AND_ROADMAP.md](../docs/ARCHITECTURE_AND_ROADMAP.md) for architecture details

---

**dataflow-api**: Where everything comes together! üéâ
