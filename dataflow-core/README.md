# dataflow-core

> **Domain Library**: Event-sourced aggregates and pure business logic

---

## üéØ **Purpose**

**dataflow-core** is a **domain library module** containing pure business logic for the DataFlow Platform. It provides:

- ‚úÖ Domain models (Commands, Events, States)
- ‚úÖ Event-sourced aggregates (PipelineAggregate)
- ‚úÖ Serialization configuration
- ‚úÖ Validation rules

**IMPORTANT**: This is a **library**, not an application!

---

## üö´ **What This Module Does NOT Contain**

- ‚ùå **NO cluster dependencies** (no pekko-cluster-typed)
- ‚ùå **NO Cassandra driver** (only Pekko Persistence API)
- ‚ùå **NO cluster configuration** (no remote.artery, seed-nodes)
- ‚ùå **NO Cassandra connection details** (no contact-points, keyspaces)
- ‚ùå **NO application runtime** (no main method, no HTTP server)

**Why?** This ensures dataflow-core can be used as a library in different contexts without forcing cluster/Cassandra dependencies on consumers.

---

## üèóÔ∏è **Architecture**

### **Dependency Hierarchy**

```
dataflow-api (APPLICATION)
    ‚îú‚îÄ‚îÄ depends on ‚Üí dataflow-sources
    ‚îú‚îÄ‚îÄ depends on ‚Üí dataflow-transforms
    ‚îú‚îÄ‚îÄ depends on ‚Üí dataflow-sinks
    ‚îî‚îÄ‚îÄ depends on ‚Üí dataflow-core (THIS MODULE)
                        ‚Üë
                        ‚îÇ
            (Pure domain library - no cluster deps)
```

### **Module Structure**

```
dataflow-core/
‚îú‚îÄ‚îÄ src/main/scala/com/dataflow/
‚îÇ   ‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/              # Domain models
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PipelineConfig.scala
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SourceConfig.scala
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SinkConfig.scala
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TransformConfig.scala
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Checkpoint.scala
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/            # Commands
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PipelineCommands.scala
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ events/              # Events
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PipelineEvents.scala
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state/               # States
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ PipelineState.scala
‚îÇ   ‚îú‚îÄ‚îÄ aggregates/              # Aggregates
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PipelineAggregate.scala
‚îÇ   ‚îî‚îÄ‚îÄ serialization/           # Serialization
‚îÇ       ‚îî‚îÄ‚îÄ CborSerializable.scala
‚îÇ
‚îî‚îÄ‚îÄ src/main/resources/
    ‚îî‚îÄ‚îÄ application.conf         # Library configuration (plugin selection only)
```

---

## üì¶ **Dependencies**

**build.sbt** (simplified):

```scala
lazy val dataflowCore = (project in file("dataflow-core"))
  .settings(
    libraryDependencies ++=
      commonDependencies ++
      testDependencies ++
      // ‚úÖ Only Pekko Persistence API (for EventSourcedBehavior)
      Seq(
        "org.apache.pekko" %% "pekko-persistence-typed"     % pekkoVersion,
        "org.apache.pekko" %% "pekko-serialization-jackson" % pekkoVersion,
        // Testing
        "org.apache.pekko" %% "pekko-persistence-testkit"   % pekkoVersion % Test,
      ) ++
      validationDependencies
  )
```

**Key Points:**
- ‚ùå **NO** `clusterDependencies` - This is not a clustered application
- ‚ùå **NO** `persistenceDependencies` - No Cassandra driver
- ‚úÖ **YES** `pekko-persistence-typed` - API for event sourcing
- ‚úÖ **YES** `pekko-serialization-jackson` - Serialization

---

## ‚öôÔ∏è **Configuration**

**application.conf** (simplified):

```hocon
# ============================================
# DataFlow Platform - Core Library Configuration
# ============================================
# Minimal configuration for the domain library
# The application module (dataflow-api) provides cluster and Cassandra config

dataflow {
  pipeline {
    default-batch-size = 1000
    max-pipelines = 100
    checkpoint-interval = 10 seconds
  }
}

pekko {
  actor {
    # Serialization configuration
    serialization-bindings {
      "com.dataflow.serialization.CborSerializable" = jackson-cbor
    }
  }

  # ‚úÖ Plugin selection ONLY (no connection details)
  persistence {
    journal.plugin = "pekko.persistence.cassandra.journal"
    snapshot-store.plugin = "pekko.persistence.cassandra.snapshot"
  }
}

# ‚ùå NO cluster configuration
# ‚ùå NO Cassandra connection details
# ‚ùå NO remote.artery configuration
```

**What's missing?**
- ‚ùå `pekko.actor.provider = cluster` - Defined in dataflow-api
- ‚ùå `pekko.remote.artery` - Defined in dataflow-api
- ‚ùå `pekko.cluster` - Defined in dataflow-api
- ‚ùå `datastax-java-driver` - Defined in dataflow-api
- ‚ùå `pekko.persistence.cassandra` (connection details) - Defined in dataflow-api

**Why?** Configuration is the responsibility of the application layer (dataflow-api).

---

## üé≠ **PipelineAggregate**

The core event-sourced aggregate managing pipeline lifecycle.

### **State Machine**

```
UninitializedState
    ‚Üì CreatePipeline
ConfiguredState
    ‚Üì StartPipeline
RunningState
    ‚Üì PausePipeline
PausedState
    ‚Üì ResumePipeline
RunningState
    ‚Üì StopPipeline
StoppedState
    ‚Üì StartPipeline (resume with checkpoint)
RunningState
```

### **Commands**

```scala
sealed trait Command
case class CreatePipeline(config: PipelineConfig, replyTo: ActorRef[Response]) extends Command
case class StartPipeline(replyTo: ActorRef[Response]) extends Command
case class StopPipeline(reason: StopReason, replyTo: ActorRef[Response]) extends Command
case class PausePipeline(reason: String, replyTo: ActorRef[Response]) extends Command
case class ResumePipeline(replyTo: ActorRef[Response]) extends Command
case class IngestBatch(records: Seq[DataRecord], replyTo: ActorRef[Response]) extends Command
case class UpdateCheckpoint(checkpoint: Checkpoint, replyTo: ActorRef[Response]) extends Command
```

### **Events**

```scala
sealed trait Event extends CborSerializable
case class PipelineCreated(id: String, name: String, config: PipelineConfig, timestamp: Instant) extends Event
case class PipelineStarted(id: String, timestamp: Instant) extends Event
case class PipelineStopped(id: String, reason: StopReason, metrics: PipelineMetrics, timestamp: Instant) extends Event
case class PipelinePaused(id: String, reason: String, timestamp: Instant) extends Event
case class PipelineResumed(id: String, timestamp: Instant) extends Event
case class BatchProcessed(id: String, recordCount: Int, checkpoint: Checkpoint, timestamp: Instant) extends Event
```

### **States**

```scala
sealed trait State
case object UninitializedState extends State
case class ConfiguredState(id: String, name: String, config: PipelineConfig, createdAt: Instant) extends State
case class RunningState(id: String, name: String, config: PipelineConfig, startedAt: Instant, checkpoint: Checkpoint) extends State
case class PausedState(id: String, name: String, config: PipelineConfig, pausedAt: Instant, checkpoint: Checkpoint) extends State
case class StoppedState(id: String, name: String, config: PipelineConfig, stoppedAt: Instant, lastCheckpoint: Checkpoint) extends State
```

---

## üîí **Type Safety**

The aggregate uses **pattern matching on (State, Event) tuples** for type-safe state transitions:

```scala
// ‚ùå OLD (WRONG - unsafe cast)
case PipelineStarted(_, ts) =>
  val cfg = state.asInstanceOf[ConfiguredState]  // Crashes if state is StoppedState!
  RunningState(...)

// ‚úÖ NEW (CORRECT - type-safe pattern matching)
(state, event) match {
  case (cfg: ConfiguredState, PipelineStarted(_, ts)) =>
    RunningState(..., checkpoint = Checkpoint.initial)

  case (stopped: StoppedState, PipelineStarted(_, ts)) =>
    RunningState(..., checkpoint = stopped.lastCheckpoint)  // Resume from checkpoint!

  case (currentState, event) =>
    log.warn("Invalid transition: {} + {}", currentState, event)
    currentState  // Don't crash
}
```

---

## üß™ **Testing**

### **Unit Tests**

```bash
sbt "project dataflow-core" test
```

Tests use `pekko-persistence-testkit` for in-memory event sourcing:

```scala
class PipelineAggregateSpec extends ScalaTestWithActorTestKit {
  "PipelineAggregate" should {
    "create pipeline from uninitialized state" in {
      val probe = testKit.createTestProbe[Response]()
      val aggregate = testKit.spawn(PipelineAggregate("test-pipeline"))

      aggregate ! CreatePipeline(config, probe.ref)
      probe.expectMessageType[PipelineCreatedResponse]
    }
  }
}
```

---

## üìö **Usage**

### **As a Library**

Other modules depend on dataflow-core:

```scala
// build.sbt
lazy val dataflowApi = (project in file("dataflow-api"))
  .dependsOn(dataflowCore % "compile->compile;test->test")
  .settings(...)
```

### **Creating an Aggregate**

```scala
import com.dataflow.aggregates.PipelineAggregate
import org.apache.pekko.cluster.sharding.typed.scaladsl.ClusterSharding

val sharding: ClusterSharding = ClusterSharding(system)

// Initialize sharding for PipelineAggregate
sharding.init(Entity(PipelineAggregate.TypeKey) { entityContext =>
  PipelineAggregate(entityContext.entityId)
})

// Send commands
val pipelineRef = sharding.entityRefFor(PipelineAggregate.TypeKey, "pipeline-123")
pipelineRef ! CreatePipeline(config, replyTo)
```

---

## ü§î **Why This Architecture?**

### **Problem Without Separation**

If dataflow-core had cluster dependencies:
```scala
// ‚ùå BAD ARCHITECTURE
lazy val dataflowCore = (project in file("dataflow-core"))
  .settings(
    libraryDependencies ++=
      clusterDependencies ++        // ‚Üê Forces cluster on all consumers
      persistenceDependencies ++    // ‚Üê Forces Cassandra client on all consumers
```

**Issues:**
- Any module depending on dataflow-core gets unwanted dependencies
- Cannot use domain logic without cluster
- Cannot test without Cassandra
- Circular dependency risk
- Violates Single Responsibility Principle

### **Solution With Separation**

```scala
// ‚úÖ GOOD ARCHITECTURE
lazy val dataflowCore = (project in file("dataflow-core"))
  .settings(
    libraryDependencies ++=
      Seq(
        "org.apache.pekko" %% "pekko-persistence-typed" % pekkoVersion,  // API only
```

**Benefits:**
- ‚úÖ Clean separation of concerns
- ‚úÖ Domain logic is reusable
- ‚úÖ Easy to test (no cluster required)
- ‚úÖ No circular dependencies
- ‚úÖ Follows Hexagonal Architecture

---

## üîó **Related Modules**

| Module | Relationship |
|--------|-------------|
| **dataflow-api** | **Application** - Runs cluster, connects to Cassandra, uses dataflow-core |
| **dataflow-sources** | **Library** - Data ingestion, depends on dataflow-core |
| **dataflow-transforms** | **Library** - Data transformation, depends on dataflow-core |
| **dataflow-sinks** | **Library** - Data output, depends on dataflow-core |

---

## üìñ **Key Concepts**

### **Event Sourcing**
State is derived from a sequence of immutable events, not stored directly.

### **CQRS**
Commands change state (write side), Projections query state (read side).

### **Aggregate**
A consistency boundary - all changes go through the aggregate.

### **Type Safety**
Pattern matching on (State, Event) ensures invalid transitions are caught at compile time.

---

## üöÄ **Next Steps**

For **running the application**, see:
- [dataflow-api/README.md](../dataflow-api/README.md) - Application module
- [Main README.md](../README.md) - Quick start guide
- [ARCHITECTURE_AND_ROADMAP.md](../docs/ARCHITECTURE_AND_ROADMAP.md) - Complete architecture

---

**dataflow-core**: Pure domain logic, zero infrastructure dependencies ‚ú®
