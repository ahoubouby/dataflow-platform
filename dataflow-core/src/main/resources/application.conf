# ============================================
# DataFlow Platform - Application Configuration
# ============================================
# Apache Pekko configuration for event-sourced distributed pipelines

# ============================================
# ACTOR SYSTEM
# ============================================
dataflow {
  # Application name
  system-name = "DataflowPlatform"

  # HTTP API settings (for future API module)
  api {
    host = "0.0.0.0"
    host = ${?DATAFLOW_API_HOST}
    port = 8080
    port = ${?DATAFLOW_API_PORT}
  }

  # Pipeline settings
  pipeline {
    # Default batch size for ingestion
    default-batch-size = 1000

    # Maximum concurrent pipelines per node
    max-pipelines = 100

    # Checkpoint frequency
    checkpoint-interval = 10 seconds
  }
}

pekko {
  # Logging configuration
  loglevel = "INFO"
  loglevel = ${?PEKKO_LOG_LEVEL}

  # Log configuration on startup
  log-config-on-start = off

  # Log dead letters
  log-dead-letters = 10
  log-dead-letters-during-shutdown = off

  # Actor system logging
  actor {
    debug {
      # Enable logging of all messages sent to an actor
      receive = off

      # Enable logging of actor lifecycle changes
      lifecycle = off

      # Enable logging of unhandled messages
      unhandled = on
    }

    # Warn about java serialization
    warn-about-java-serializer-usage = on
    allow-java-serialization = off

    # Serialization configuration
    serializers {
      jackson-json = "org.apache.pekko.serialization.jackson.JacksonJsonSerializer"
      jackson-cbor = "org.apache.pekko.serialization.jackson.JacksonCborSerializer"
    }

    serialization-bindings {
      "com.dataflow.serialization.CborSerializable" = jackson-cbor
    }

    # Provider for clustering
    provider = cluster
  }

  # ============================================
  # REMOTE / CLUSTER COMMUNICATION
  # ============================================
  remote {
    artery {
      enabled = on
      transport = tcp

      canonical {
        hostname = "127.0.0.1"
        hostname = ${?PEKKO_REMOTE_HOST}
        port = 2551
        port = ${?PEKKO_REMOTE_PORT}
      }

      # Advanced tuning
      advanced {
        # Maximum size of messages that can be sent
        maximum-frame-size = 512 KiB

        # Buffer size for outbound messages
        outbound-message-queue-size = 3072

        # Compression settings
        compression {
          actor-refs {
            max = 256
          }
          manifests {
            max = 256
          }
        }
      }
    }
  }

  # ============================================
  # CLUSTER CONFIGURATION
  # ============================================
  cluster {
    # Seed nodes for cluster formation
    seed-nodes = [
      "pekko://DataflowPlatform@127.0.0.1:2551"
    ]
    seed-nodes = ${?PEKKO_CLUSTER_SEED_NODES}

    # Roles for this node
    roles = ["pipeline-processor"]

    # Downing provider for handling split-brain scenarios
    downing-provider-class = "org.apache.pekko.cluster.sbr.SplitBrainResolverProvider"

    # Split Brain Resolver configuration
    split-brain-resolver {
      # Strategy: static-quorum, keep-majority, keep-oldest, keep-referee
      active-strategy = keep-majority

      # Time margin for downing decisions
      stable-after = 20s

      # Down all nodes if cluster is smaller than this
      down-all-when-unstable = on
    }

    # Failure detector settings
    failure-detector {
      # Threshold for failure detection
      threshold = 12.0

      # Expected response time
      acceptable-heartbeat-pause = 10s

      # Heartbeat interval
      heartbeat-interval = 2s
    }

    # Cluster sharding configuration
    sharding {
      # Number of shards
      number-of-shards = 100

      # Timeout for asking the coordinator for shard location
      coordinator-singleton-lease-retry-interval = 5s

      # Remember entities settings
      remember-entities = on
      remember-entities-store = "eventsourced"

      # Passivation settings
      passivate-idle-entity-after = 2 hours

      # Rebalancing
      least-shard-allocation-strategy {
        rebalance-threshold = 10
        max-simultaneous-rebalance = 3
      }
    }

    # Cluster singleton settings
    singleton {
      # Minimum number of cluster members before singleton is started
      min-number-of-members = 1

      # Singleton role (empty = all nodes can host singleton)
      role = ""

      # Timeout for handover
      hand-over-retry-interval = 1s
    }

    # Distributed data settings
    distributed-data {
      # Role of nodes to use for distributed data
      role = ""

      # How often to gossip
      gossip-interval = 2s

      # How long to wait for majority ack
      notify-subscribers-interval = 500ms
    }
  }

  # ============================================
  # PERSISTENCE CONFIGURATION
  # ============================================
  persistence {
    # Journal plugin
    journal {
      plugin = "pekko.persistence.cassandra.journal"

      # Auto-start journal on system startup
      auto-start-journals = ["pekko.persistence.cassandra.journal"]
    }

    # Snapshot store plugin
    snapshot-store {
      plugin = "pekko.persistence.cassandra.snapshot"

      # Auto-start snapshot store on system startup
      auto-start-snapshot-stores = ["pekko.persistence.cassandra.snapshot"]
    }

    # Event adapter configuration
    # Used for event versioning and migration
    # event-adapters {
    #   pipeline-tagger = "com.dataflow.adapters.PipelineEventTagger"
    # }

    # event-adapter-bindings {
    #   "com.dataflow.domain.events.PipelineEvent" = pipeline-tagger
    # }

    # At-least-once delivery settings
    at-least-once-delivery {
      redeliver-interval = 5s
      redelivery-burst-limit = 10000
      warn-after-number-of-unconfirmed-attempts = 5
      max-unconfirmed-messages = 100000
    }

    # Failure recovery settings
    recovery {
      # Maximum number of events to replay per batch
      max-events-per-batch = 1000
    }
  }

  # ============================================
  # CASSANDRA PLUGIN CONFIGURATION
  # ============================================
  persistence.cassandra {
    # Journal configuration
    journal {
      # Keyspace name
      keyspace = "dataflow_journal"

      # Table name
      table = "messages"

      # Replication strategy
      replication-strategy = "SimpleStrategy"
      replication-factor = 1

      # Write consistency level
      write-consistency = "QUORUM"

      # Read consistency level
      read-consistency = "QUORUM"

      # Enable events by tag for projections
      event-processor {
        # Number of tag writers
        # Increase for better throughput with projections
        tag-write-parallelism = 4
      }
    }

    # Snapshot configuration
    snapshot {
      # Keyspace name
      keyspace = "dataflow_snapshot"

      # Table name
      table = "snapshots"

      # Replication strategy
      replication-strategy = "SimpleStrategy"
      replication-factor = 1

      # Write consistency level
      write-consistency = "ONE"

      # Read consistency level
      read-consistency = "ONE"
    }

    # Query configuration
    query {
      # Refresh interval for live queries
      refresh-interval = 3s

      # Maximum buffer size
      max-buffer-size = 500

      # Maximum result size for queries
      max-result-size = 50000
    }

    # Session settings
    session-provider = "pekko.persistence.cassandra.ConfigSessionProvider"

    # Cassandra connection settings
    session {
      # Contact points
      contact-points = ["127.0.0.1:9042"]
      contact-points = ${?CASSANDRA_CONTACT_POINTS}

      # Local datacenter
      local-datacenter = "datacenter1"
      local-datacenter = ${?CASSANDRA_LOCAL_DC}

      # Credentials (if authentication is enabled)
      # authentication {
      #   username = "cassandra"
      #   username = ${?CASSANDRA_USERNAME}
      #   password = "cassandra"
      #   password = ${?CASSANDRA_PASSWORD}
      # }

      # Connection settings
      connection {
        # Connection timeout
        connect-timeout = 5s

        # Request timeout
        request-timeout = 10s

        # Pool settings
        pool {
          # Number of connections per host
          local.size = 2
        }
      }
    }

    # Keyspace auto-creation (disable in production)
    keyspace-autocreate = true
    keyspace-autocreate = ${?CASSANDRA_KEYSPACE_AUTOCREATE}

    # Table auto-creation (disable in production)
    tables-autocreate = true
    tables-autocreate = ${?CASSANDRA_TABLES_AUTOCREATE}
  }

  # ============================================
  # PROJECTION CONFIGURATION (for future use)
  # ============================================
  projection {
    # Cassandra offset store
    cassandra {
      offset-store {
        keyspace = "dataflow_projections"
        table = "offsets"
      }
    }

    # Restart backoff settings
    restart-backoff {
      min-backoff = 3s
      max-backoff = 30s
      random-factor = 0.2
    }

    # Recovery strategy
    recovery-strategy {
      strategy = fail
      retries = 5
      retry-delay = 1s
    }
  }

  # ============================================
  # COORDINATED SHUTDOWN
  # ============================================
  coordinated-shutdown {
    # Timeout for each shutdown phase
    default-phase-timeout = 10s

    # Phases configuration
    phases {
      # Custom phase for graceful pipeline shutdown
      before-service-unbind {
        timeout = 30s
        depends-on = [cluster-sharding-shutdown-region]
      }
    }
  }

  # ============================================
  # MANAGEMENT (for Pekko Management in future)
  # ============================================
  # management {
  #   http {
  #     hostname = "0.0.0.0"
  #     port = 8558
  #
  #     route-providers-read-only = false
  #   }
  #
  #   cluster {
  #     bootstrap {
  #       contact-point-discovery {
  #         service-name = "dataflow-platform"
  #         discovery-method = config
  #       }
  #     }
  #   }
  #
  #   health-checks {
  #     readiness-checks {
  #       cluster-membership = "org.apache.pekko.management.cluster.scaladsl.ClusterMembershipCheck"
  #     }
  #
  #     liveness-checks {
  #       cluster-membership = "org.apache.pekko.management.cluster.scaladsl.ClusterMembershipCheck"
  #     }
  #   }
  # }
}

# ============================================
# PEKKO HTTP CONFIGURATION (for future API module)
# ============================================
pekko.http {
  server {
    # Server listening interface
    interface = "0.0.0.0"

    # Request timeout
    request-timeout = 20s

    # Idle timeout
    idle-timeout = 60s

    # Maximum connections
    max-connections = 1024

    # Backlog size
    backlog = 100

    # Parsing settings
    parsing {
      max-content-length = 8m
      max-uri-length = 2k
    }
  }

  client {
    # Connection timeout
    connecting-timeout = 10s

    # Idle timeout
    idle-timeout = 60s
  }

  host-connection-pool {
    # Maximum connections per host
    max-connections = 4

    # Maximum open requests
    max-open-requests = 32

    # Idle timeout
    idle-timeout = 30s
  }
}

# ============================================
# LOGGING BACKEND (Logback)
# ============================================
# Additional configuration in logback.xml

# ============================================
# METRICS AND MONITORING (Kamon)
# ============================================
kamon {
  # Enable/disable Kamon
  enabled = true
  enabled = ${?KAMON_ENABLED}

  # Environment information
  environment {
    service = "dataflow-platform"
    service = ${?KAMON_SERVICE_NAME}

    host = "auto"
    host = ${?KAMON_HOST}

    instance = "auto"
    instance = ${?KAMON_INSTANCE}
  }

  # Metric collection settings
  metric {
    # How often to collect metrics
    tick-interval = 10 seconds

    # Optimistic tick alignment
    optimistic-tick-alignment = yes

    # Filters for metrics
    filters {
      # Accept all metrics by default
      dataflow {
        includes = ["dataflow.**"]
        excludes = []
      }

      # Akka/Pekko actor metrics
      akka-actor {
        includes = ["akka.actor.**", "pekko.actor.**"]
        excludes = []
      }

      # System metrics
      system {
        includes = ["system.**", "process.**", "jvm.**"]
        excludes = []
      }
    }
  }

  # Prometheus reporter configuration
  prometheus {
    # Start the embedded HTTP server for Prometheus scraping
    start-embedded-http-server = yes
    start-embedded-http-server = ${?KAMON_PROMETHEUS_EMBEDDED}

    # Port for Prometheus metrics endpoint
    embedded-server {
      hostname = "0.0.0.0"
      hostname = ${?KAMON_PROMETHEUS_HOST}

      port = 9095
      port = ${?KAMON_PROMETHEUS_PORT}
    }

    # Default buckets for histograms (processing time, etc.)
    buckets {
      default-buckets = [
        1,
        5,
        10,
        25,
        50,
        75,
        100,
        250,
        500,
        750,
        1000,
        2500,
        5000,
        7500,
        10000
      ]

      time-buckets = [
        0.001,
        0.005,
        0.01,
        0.025,
        0.05,
        0.075,
        0.1,
        0.25,
        0.5,
        0.75,
        1.0,
        2.5,
        5.0,
        7.5,
        10.0
      ]
    }
  }

  # System metrics (CPU, memory, GC)
  system-metrics {
    # Enable JVM metrics
    jvm {
      enabled = yes

      # Collect heap metrics
      heap {
        enabled = yes
      }

      # Collect non-heap metrics
      non-heap {
        enabled = yes
      }

      # Collect garbage collection metrics
      garbage-collection {
        enabled = yes
      }

      # Collect thread metrics
      threads {
        enabled = yes
      }
    }

    # Enable process metrics (CPU, file descriptors)
    process {
      enabled = yes
    }

    # Enable host metrics (CPU, memory, network, disk)
    host {
      enabled = no  # Requires sigar native library
      enabled = ${?KAMON_HOST_METRICS_ENABLED}
    }
  }

  # Instrumentation modules
  instrumentation {
    # Akka/Pekko actor instrumentation
    akka {
      # Track actor mailbox sizes
      actor {
        tracking {
          mailbox-size = yes
          time-in-mailbox = yes
          processing-time = yes
        }
      }

      # Track router metrics
      router {
        tracking {
          messages = yes
          time-in-mailbox = yes
          processing-time = yes
        }
      }
    }

    # Future/Promise instrumentation
    scala {
      futures {
        enabled = yes
      }
    }
  }

  # Module configuration
  modules {
    # Kamon Status Page (debug endpoint)
    status-page {
      enabled = yes
      enabled = ${?KAMON_STATUS_PAGE_ENABLED}

      listen {
        hostname = "0.0.0.0"
        port = 5266
      }
    }
  }

  # Trace configuration (optional - for distributed tracing)
  trace {
    # Sampling rate (0.0 to 1.0)
    sampler = "always"  # Use "random" or "adaptive" in production

    # Join remote traces
    join-remote-parents-with-same-span-id = yes

    # Trace ID length (8 or 16 bytes)
    identifier-scheme = "double"
  }
}

# ============================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ============================================
# Override any setting using environment variables:
#
# PEKKO_LOG_LEVEL=DEBUG
# PEKKO_REMOTE_HOST=192.168.1.100
# PEKKO_REMOTE_PORT=2551
# CASSANDRA_CONTACT_POINTS=["cassandra1:9042","cassandra2:9042"]
# CASSANDRA_KEYSPACE_AUTOCREATE=false
# CASSANDRA_TABLES_AUTOCREATE=false
#
# Or provide application-prod.conf, application-dev.conf, etc.
# and use: -Dconfig.resource=application-prod.conf

# ============================================
# DEVELOPMENT MODE
# ============================================
# For local development, include development.conf:
# include "development"

# ============================================
# PRODUCTION MODE
# ============================================
# For production, include production.conf:
# include "production"
